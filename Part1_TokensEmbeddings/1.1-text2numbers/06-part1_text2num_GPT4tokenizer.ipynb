{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnNdFahh7-8z"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Exploring ChatGPT4's tokenizer<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8JzcByJ_dVt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib defaults\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-rylt4G8MWp"
   },
   "outputs": [],
   "source": [
    "# need to install the tiktoken library to get OpenAI's tokenizer\n",
    "# note: it's tik-token, not tiktok-en :P\n",
    "!pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vll_zH5t_gHC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f34QAbA-_gD-"
   },
   "outputs": [],
   "source": [
    "# GPT-4's tokenizer\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ehv3aujvgh"
   },
   "outputs": [],
   "source": [
    "# get help\n",
    "tokenizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3wREWipBBH0"
   },
   "outputs": [],
   "source": [
    "# vocab size\n",
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIox41g_BBFS"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQex4QV7ZC8D"
   },
   "outputs": [],
   "source": [
    "# but not all tokens are valid, e.g.,\n",
    "print(tokenizer.n_vocab)\n",
    "tokenizer.decode([100261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaGtXMdoZDAl"
   },
   "outputs": [],
   "source": [
    "# list of all tokens:\n",
    "# https://github.com/vnglst/gpt4-tokens/blob/main/decode-tokens.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTBxIVVoBBCw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UiNQED81ig"
   },
   "source": [
    "# Explore some tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKLYdHUiLalD"
   },
   "outputs": [],
   "source": [
    "for i in range(1000,1050):\n",
    "  print(f'{i} = {tokenizer.decode([i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-0XBTjgLahX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W54bWoRcF6fd"
   },
   "source": [
    "# Tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQoW6wxE_gBI"
   },
   "outputs": [],
   "source": [
    "text = \"My name is Mike and I like toothpaste-flavored chocolate.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVemggKqXyQL"
   },
   "outputs": [],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wt9WuiI_f-U"
   },
   "outputs": [],
   "source": [
    "for word in text.split():\n",
    "  print(f'\"{word}\" comprises token(s) {tokenizer.encode(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwUd-Jry_f7o"
   },
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "  print(f'Token {t:>6} is \"{tokenizer.decode([t])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V86VfNz3_f4y"
   },
   "outputs": [],
   "source": [
    "# with special (non-ASCII) characters\n",
    "tokenizer.encode('â')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73ZpWc09F2yT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v28C8R_gF4XK"
   },
   "source": [
    "# How long are the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN2EMOYi6ouB"
   },
   "outputs": [],
   "source": [
    "# initialize lengths vector\n",
    "token_lengths = np.zeros(tokenizer.n_vocab)\n",
    "\n",
    "# get the number of characters in each token\n",
    "for idx in range(tokenizer.n_vocab):\n",
    "  try:\n",
    "    token_lengths[idx] = len(tokenizer.decode([idx]))\n",
    "  except:\n",
    "    token_lengths[idx] = np.nan\n",
    "\n",
    "# count unique lengths\n",
    "uniqueLengths,tokenCount = np.unique(token_lengths,return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "# visualize\n",
    "_,axs = plt.subplots(1,2,figsize=(12,4))\n",
    "axs[0].plot(token_lengths,'k.',markersize=3,alpha=.4)\n",
    "axs[0].set(xlim=[0,tokenizer.n_vocab],xlabel='Token index',ylabel='Token length (characters)',\n",
    "           title='GPT4 token lengths')\n",
    "\n",
    "axs[1].bar(uniqueLengths,tokenCount,color='k',edgecolor='gray')\n",
    "axs[1].set(xlim=[0,max(uniqueLengths)],xlabel='Token length (chars)',ylabel='Token count (log scale)',\n",
    "           title='Distribution of token lengths')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9WjkvGzDa65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shSBSrGSoHL3"
   },
   "source": [
    "# Many word-tokens start with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q18PZz3oK1j"
   },
   "outputs": [],
   "source": [
    "# single-token words with vs. without spaces\n",
    "print( tokenizer.encode(' Michael') )\n",
    "print( tokenizer.encode('Michael') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul4qzL0VoUqP"
   },
   "outputs": [],
   "source": [
    "# multi-token words without a space\n",
    "print( tokenizer.encode(' Peach') )\n",
    "print( tokenizer.encode('Peach') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-YLajcooHJP"
   },
   "outputs": [],
   "source": [
    "peach = tokenizer.encode('Peach')\n",
    "[tokenizer.decode([p]) for p in peach]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41SHGKIsoHGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsfMaYxen8oU"
   },
   "source": [
    "# The Time Machine book encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-oWizBu_fzO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n",
    "\n",
    "# split by punctuation\n",
    "words = re.split(r'([,.:;—?_!\"“()\\']|--|\\s)',text)\n",
    "words = [item.strip() for item in words if item.strip()]\n",
    "print(f'There are {len(words)} words.')\n",
    "words[10000:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLR0jdSclljl"
   },
   "outputs": [],
   "source": [
    "# tokens of a random word in the text\n",
    "someRandomWord = np.random.choice(words)\n",
    "print(f'\"{someRandomWord}\" has token {tokenizer.encode(someRandomWord)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D39CEFi__fwi"
   },
   "outputs": [],
   "source": [
    "for t in words[:20]:\n",
    "  print(f'\"{t}\" has {len(tokenizer.encode(t))} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkP7Fdcg_ftn"
   },
   "outputs": [],
   "source": [
    "for spelling in ['book','Book','bOok']:\n",
    "  print(f'\"{spelling}\" has tokens {tokenizer.encode(spelling)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_8A_ey6r8qL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVvhTKdDHISD"
   },
   "source": [
    "# But do we need to separate the text into words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntdIA6qMr8lk"
   },
   "outputs": [],
   "source": [
    "# what happens if we just tokenize the raw (unprocessed) text?\n",
    "tmTokens = tokenizer.encode(text)\n",
    "print(f'The text has {len(tmTokens):,} tokens and {len(words):,} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xQRRWpfHQG2"
   },
   "outputs": [],
   "source": [
    "# check out some tokens\n",
    "\n",
    "for t in tmTokens[9990:10020]:\n",
    "  print(f'Token {t:>6}: \"{tokenizer.decode([t])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE4HmJ8JBXCB"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tmTokens[9990:10020]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZdw1sP8IVuz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5pyA9+pTsYfvQ6VmRqRSO",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
