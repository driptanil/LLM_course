{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddingss<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Byte-pair encoding<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ],
   "metadata": {
    "id": "Q8_-VKZNDmzB"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "APFQtXa0brAf",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.662888Z",
     "start_time": "2025-12-22T19:05:11.622079Z"
    }
   },
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TrCWDXUdTS_N",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.668836Z",
     "start_time": "2025-12-22T19:05:11.667564Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize the vocabulary"
   ],
   "metadata": {
    "id": "bRwQVVLT6ZNP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# some text with lots of repetitions\n",
    "text = 'like liker love lovely hug hugs hugging hearts'\n",
    "\n",
    "chars = list(set(text))\n",
    "chars.sort() # initial vocab is sorted\n",
    "\n",
    "for l in chars:\n",
    "  print(f'\"{l}\" appears {text.count(l)} times.')"
   ],
   "metadata": {
    "id": "6M7HN5nLTmaq",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.673422Z",
     "start_time": "2025-12-22T19:05:11.671474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" appears 7 times.\n",
      "\"a\" appears 1 times.\n",
      "\"e\" appears 5 times.\n",
      "\"g\" appears 5 times.\n",
      "\"h\" appears 4 times.\n",
      "\"i\" appears 3 times.\n",
      "\"k\" appears 2 times.\n",
      "\"l\" appears 5 times.\n",
      "\"n\" appears 1 times.\n",
      "\"o\" appears 2 times.\n",
      "\"r\" appears 2 times.\n",
      "\"s\" appears 2 times.\n",
      "\"t\" appears 1 times.\n",
      "\"u\" appears 3 times.\n",
      "\"v\" appears 2 times.\n",
      "\"y\" appears 1 times.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# make a vocabulary\n",
    "vocab = { word:i for i,word in enumerate(chars) }\n",
    "vocab"
   ],
   "metadata": {
    "id": "mXFIcoLk7Q8F",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.682103Z",
     "start_time": "2025-12-22T19:05:11.677449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'e': 2,\n",
       " 'g': 3,\n",
       " 'h': 4,\n",
       " 'i': 5,\n",
       " 'k': 6,\n",
       " 'l': 7,\n",
       " 'n': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 's': 11,\n",
       " 't': 12,\n",
       " 'u': 13,\n",
       " 'v': 14,\n",
       " 'y': 15}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# the text needs to be a list, not a string\n",
    "# each element in the list is a token\n",
    "origtext = list(text)\n",
    "print(text)\n",
    "print(origtext)"
   ],
   "metadata": {
    "id": "k70QKgFkBCr2",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.688593Z",
     "start_time": "2025-12-22T19:05:11.686773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like liker love lovely hug hugs hugging hearts\n",
      "['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "4NW4pLnVUgcd",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.703836Z",
     "start_time": "2025-12-22T19:05:11.701724Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find character pairs and merge the most frequent"
   ],
   "metadata": {
    "id": "K_wH9S9WUgZF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "token_pairs = dict()\n",
    "\n",
    "# loop over tokens\n",
    "for i in range(len(origtext)-1):\n",
    "\n",
    "  # create a pair\n",
    "  pair = origtext[i] + origtext[i+1]\n",
    "\n",
    "  # increase pair frequencies\n",
    "  if pair in token_pairs:\n",
    "    token_pairs[pair] += 1\n",
    "  else:\n",
    "    token_pairs[pair] = 1\n",
    "\n",
    "token_pairs"
   ],
   "metadata": {
    "id": "porxIPE4cqrZ",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.713647Z",
     "start_time": "2025-12-22T19:05:11.710903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'li': 2,\n",
       " 'ik': 2,\n",
       " 'ke': 2,\n",
       " 'e ': 2,\n",
       " ' l': 3,\n",
       " 'er': 1,\n",
       " 'r ': 1,\n",
       " 'lo': 2,\n",
       " 'ov': 2,\n",
       " 've': 2,\n",
       " 'el': 1,\n",
       " 'ly': 1,\n",
       " 'y ': 1,\n",
       " ' h': 4,\n",
       " 'hu': 3,\n",
       " 'ug': 3,\n",
       " 'g ': 2,\n",
       " 'gs': 1,\n",
       " 's ': 1,\n",
       " 'gg': 1,\n",
       " 'gi': 1,\n",
       " 'in': 1,\n",
       " 'ng': 1,\n",
       " 'he': 1,\n",
       " 'ea': 1,\n",
       " 'ar': 1,\n",
       " 'rt': 1,\n",
       " 'ts': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# find the most frequent pair\n",
    "mostFreqPair_idx = np.argmax(list(token_pairs.values()))\n",
    "mostFreqPair_char = list(token_pairs.keys())[mostFreqPair_idx]\n",
    "print(f'The most frequent character pair is \"{mostFreqPair_char}\" with {list(token_pairs.values())[mostFreqPair_idx]} appearances')"
   ],
   "metadata": {
    "id": "uFaRRbskcqn-",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.726357Z",
     "start_time": "2025-12-22T19:05:11.723976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent character pair is \" h\" with 4 appearances\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# update the vocab\n",
    "vocab[mostFreqPair_char] = max(vocab.values())+1\n",
    "vocab"
   ],
   "metadata": {
    "id": "RWWoz2T-FWcE",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.732606Z",
     "start_time": "2025-12-22T19:05:11.730292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'e': 2,\n",
       " 'g': 3,\n",
       " 'h': 4,\n",
       " 'i': 5,\n",
       " 'k': 6,\n",
       " 'l': 7,\n",
       " 'n': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 's': 11,\n",
       " 't': 12,\n",
       " 'u': 13,\n",
       " 'v': 14,\n",
       " 'y': 15,\n",
       " ' h': 16}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "md3EG369FgmL",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.738697Z",
     "start_time": "2025-12-22T19:05:11.737549Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Replace the token pair with one token"
   ],
   "metadata": {
    "id": "um8UTajPFght"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# initialize a new text list\n",
    "newtext = []\n",
    "\n",
    "# loop through the list\n",
    "i = 0\n",
    "while i<(len(origtext)-1):\n",
    "\n",
    "  # test whether the pair of this and the following elements match the newly-created pair\n",
    "  if (origtext[i]+origtext[i+1]) == mostFreqPair_char:\n",
    "\n",
    "    # append to the new version of the text\n",
    "    newtext.append(mostFreqPair_char)\n",
    "    print(f'added \"{mostFreqPair_char}\"')\n",
    "\n",
    "    # skip the next character\n",
    "    i += 2\n",
    "\n",
    "  # this isn't a merged pair, so add this token to the list\n",
    "  else:\n",
    "    newtext.append(origtext[i])\n",
    "\n",
    "    # move to the next character\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(f'Original text: {origtext}')\n",
    "print(f'Updated text:  {newtext}')\n",
    "\n",
    "print(f'\\n\\nOriginal text had {len(origtext)} tokens; new text has {len(newtext)} tokens.')"
   ],
   "metadata": {
    "id": "9C4p87b66hMr",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.745546Z",
     "start_time": "2025-12-22T19:05:11.743273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added \" h\"\n",
      "added \" h\"\n",
      "added \" h\"\n",
      "added \" h\"\n",
      "\n",
      "\n",
      "Original text: ['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n",
      "Updated text:  ['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' h', 'u', 'g', ' h', 'u', 'g', 's', ' h', 'u', 'g', 'g', 'i', 'n', 'g', ' h', 'e', 'a', 'r', 't']\n",
      "\n",
      "\n",
      "Original text had 46 tokens; new text has 41 tokens.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "T7oJ4dgs6hPe",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.753570Z",
     "start_time": "2025-12-22T19:05:11.752232Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the most common letter pairs (again!)"
   ],
   "metadata": {
    "id": "axWcmSB76hSR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "token_pairs = dict()\n",
    "\n",
    "# loop over the newtext tokens (not the original!)\n",
    "for i in range(len(newtext)-1):\n",
    "\n",
    "  # create a pair\n",
    "  pair = newtext[i] + newtext[i+1]\n",
    "\n",
    "  # increase pair frequencies\n",
    "  if pair in token_pairs:\n",
    "    token_pairs[pair] += 1\n",
    "  else:\n",
    "    token_pairs[pair] = 1\n",
    "\n",
    "token_pairs"
   ],
   "metadata": {
    "id": "OTmE2Axj6hWv",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.759234Z",
     "start_time": "2025-12-22T19:05:11.756902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'li': 2,\n",
       " 'ik': 2,\n",
       " 'ke': 2,\n",
       " 'e ': 2,\n",
       " ' l': 3,\n",
       " 'er': 1,\n",
       " 'r ': 1,\n",
       " 'lo': 2,\n",
       " 'ov': 2,\n",
       " 've': 2,\n",
       " 'el': 1,\n",
       " 'ly': 1,\n",
       " 'y h': 1,\n",
       " ' hu': 3,\n",
       " 'ug': 3,\n",
       " 'g h': 2,\n",
       " 'gs': 1,\n",
       " 's h': 1,\n",
       " 'gg': 1,\n",
       " 'gi': 1,\n",
       " 'in': 1,\n",
       " 'ng': 1,\n",
       " ' he': 1,\n",
       " 'ea': 1,\n",
       " 'ar': 1,\n",
       " 'rt': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Ky9njLdZUgVR",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.767726Z",
     "start_time": "2025-12-22T19:05:11.766278Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now using functions"
   ],
   "metadata": {
    "id": "4Smkdx0MUgRx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# > -----------------------------------\n",
    "def get_pair_stats(text2pair):\n",
    "  token_pairs = dict()\n",
    "\n",
    "  # loop over tokens\n",
    "  for i in range(len(text2pair)-1):\n",
    "\n",
    "    # create a pair\n",
    "    pair = text2pair[i] + text2pair[i+1]\n",
    "\n",
    "    # increase pair frequencies\n",
    "    if pair in token_pairs:\n",
    "      token_pairs[pair] += 1\n",
    "    else:\n",
    "      token_pairs[pair] = 1\n",
    "\n",
    "  return token_pairs\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# > -----------------------------------\n",
    "def update_vocab(token_pairs,vocab):\n",
    "\n",
    "  # find the most frequent pair\n",
    "  idx = np.argmax(list(token_pairs.values()))\n",
    "  newtok = list(token_pairs.keys())[idx]\n",
    "\n",
    "  # update the vocab\n",
    "  vocab[newtok] = max(vocab.values())+1\n",
    "  return vocab,newtok\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "def generate_new_token_seq(prevtext,newtoken):\n",
    "\n",
    "  # initialize a new text list\n",
    "  newtext = []\n",
    "\n",
    "  # loop through the list\n",
    "  i = 0\n",
    "  while i<(len(prevtext)-1):\n",
    "\n",
    "    # test whether the pair of this and the following elements match the newly-created pair\n",
    "    if (prevtext[i]+prevtext[i+1]) == newtoken:\n",
    "      newtext.append(newtoken)\n",
    "      i += 2 # skip the next character\n",
    "\n",
    "    # not a pair\n",
    "    else:\n",
    "      newtext.append(prevtext[i])\n",
    "      i += 1 # move to the next character\n",
    "\n",
    "\n",
    "\n",
    "  # the following code was missing from the video\n",
    "  if i < len(prevtext):\n",
    "    newtext.append(prevtext[i])\n",
    "\n",
    "  return newtext\n",
    "# ----------------------------------- <"
   ],
   "metadata": {
    "id": "DrX7ThhHcqk3",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.773002Z",
     "start_time": "2025-12-22T19:05:11.770251Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GaBv1eDocqhv",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.776875Z",
     "start_time": "2025-12-22T19:05:11.775740Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# re-initialize the vocab\n",
    "vocab = { word:i for i,word in enumerate(chars) }\n",
    "print(f'Vocab has {len(vocab)} tokens.')"
   ],
   "metadata": {
    "id": "jVnWU2qMcqe4",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.781734Z",
     "start_time": "2025-12-22T19:05:11.779892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab has 16 tokens.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "r4vd-Pn1KjpS",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.789165Z",
     "start_time": "2025-12-22T19:05:11.788041Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## do one iteration\n",
    "\n",
    "# find and count pairs\n",
    "pairs = get_pair_stats(origtext)\n",
    "\n",
    "# update the dictionary\n",
    "vocab,newtoken = update_vocab(pairs,vocab)\n",
    "\n",
    "# get a new list of tokens\n",
    "updated_text = generate_new_token_seq(origtext,newtoken)\n",
    "print(f'Vocab has {len(vocab)} tokens.')"
   ],
   "metadata": {
    "id": "AJJoz6Y8KcPR",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.793681Z",
     "start_time": "2025-12-22T19:05:11.791856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab has 17 tokens.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "updated_text"
   ],
   "metadata": {
    "id": "IfVQp-FkoWmW",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.801729Z",
     "start_time": "2025-12-22T19:05:11.799613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'l',\n",
       " 'y',\n",
       " ' h',\n",
       " 'u',\n",
       " 'g',\n",
       " ' h',\n",
       " 'u',\n",
       " 'g',\n",
       " 's',\n",
       " ' h',\n",
       " 'u',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' h',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 't',\n",
       " 's']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "## do a second iteration\n",
    "pairs = get_pair_stats(updated_text)\n",
    "\n",
    "# update the dictionary\n",
    "vocab,newtoken = update_vocab(pairs,vocab)\n",
    "\n",
    "# get a new list of tokens\n",
    "updated_text = generate_new_token_seq(updated_text,newtoken)\n",
    "print(f'Vocab has {len(vocab)} tokens.')"
   ],
   "metadata": {
    "id": "72G7ZJ1fcqbw",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.808948Z",
     "start_time": "2025-12-22T19:05:11.807255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab has 18 tokens.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "vocab"
   ],
   "metadata": {
    "id": "8fzm86CAcqYY",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.816838Z",
     "start_time": "2025-12-22T19:05:11.814988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'e': 2,\n",
       " 'g': 3,\n",
       " 'h': 4,\n",
       " 'i': 5,\n",
       " 'k': 6,\n",
       " 'l': 7,\n",
       " 'n': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 's': 11,\n",
       " 't': 12,\n",
       " 'u': 13,\n",
       " 'v': 14,\n",
       " 'y': 15,\n",
       " ' h': 16,\n",
       " ' l': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8zwgt0JTNw0y",
    "ExecuteTime": {
     "end_time": "2025-12-22T19:05:11.824522Z",
     "start_time": "2025-12-22T19:05:11.823462Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
