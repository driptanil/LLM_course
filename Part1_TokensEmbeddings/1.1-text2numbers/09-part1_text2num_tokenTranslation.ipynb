{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "232N0bmJYzDv"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens and embeddings<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Translating between tokenizers<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:17:47.667279Z",
     "start_time": "2025-12-22T19:17:47.666094Z"
    },
    "id": "Tc_Ga6hAYy1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XnNp0fLuqyh"
   },
   "source": [
    "# Import two tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.219424Z",
     "start_time": "2025-12-22T19:17:47.670146Z"
    },
    "id": "wJfxECXYusgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GPT4\n",
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "gpt4Tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer\n",
    "bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.224004Z",
     "start_time": "2025-12-22T19:18:03.222734Z"
    },
    "id": "d_gvDTW-uqvf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvlXr72BaHG_"
   },
   "source": [
    "# Attempting a direct translation from GPT4 to BERT and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.230594Z",
     "start_time": "2025-12-22T19:18:03.226850Z"
    },
    "id": "MVAkfwC6aMFW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text:\n",
      "Hello, my name is Mike and I like purple.\n",
      "\n",
      "\n",
      "GPT4 tokens:\n",
      "[9906, 11, 856, 836, 374, 11519, 323, 358, 1093, 25977, 13]\n",
      "\n",
      "Decoded using GPT4:\n",
      "Hello, my name is Mike and I like purple.\n",
      "\n",
      "Decoded using BERT:\n",
      "lately [unused10] [unused851] [unused831] [unused369] decent [unused318] [unused353] ¾ olympian [unused12]\n",
      "\n",
      "\n",
      "BERT tokens:\n",
      "[101, 7592, 1010, 2026, 2171, 2003, 3505, 1998, 1045, 2066, 6379, 1012, 102]\n",
      "\n",
      "Decoded using BERT:\n",
      "[CLS] hello, my name is mike and i like purple. [SEP]\n",
      "\n",
      "Decoded using GPT4:\n",
      "�.deleteceptionrgretoin\\\\(idate\tfor\tendinclude�\n"
     ]
    }
   ],
   "source": [
    "# issue is that they have different tokenizers, so needs to be translated into text and re-tokenized\n",
    "startingtext = 'Hello, my name is Mike and I like purple.'\n",
    "\n",
    "# GPT4's tokens:\n",
    "gpt4Toks = gpt4Tokenizer.encode(startingtext)\n",
    "\n",
    "# bert's tokens\n",
    "bertToks = bertTokenizer.encode(startingtext)\n",
    "\n",
    "print(f'Starting text:\\n{startingtext}')\n",
    "print(f'\\n\\nGPT4 tokens:\\n{gpt4Toks}')\n",
    "print(f\"\\nDecoded using GPT4:\\n{gpt4Tokenizer.decode(gpt4Toks)}\")\n",
    "print(f\"\\nDecoded using BERT:\\n{bertTokenizer.decode(gpt4Toks)}\")\n",
    "\n",
    "print(f'\\n\\nBERT tokens:\\n{bertToks}')\n",
    "print(f\"\\nDecoded using BERT:\\n{bertTokenizer.decode(bertToks)}\")\n",
    "print(f\"\\nDecoded using GPT4:\\n{gpt4Tokenizer.decode(bertToks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.239330Z",
     "start_time": "2025-12-22T19:18:03.237960Z"
    },
    "id": "zT0HB-xH4PVP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3zbVO5XadpI"
   },
   "source": [
    "# The right way to translate (numbers to text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.245093Z",
     "start_time": "2025-12-22T19:18:03.241532Z"
    },
    "id": "NuScEgeVay4C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, my name is mike and i like purple. [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text -> GPT4 tokens -> text -> BERT tokens\n",
    "\n",
    "# 1) to GPT4 tokens\n",
    "startingtext = 'Hello, my name is Mike and I like purple.'\n",
    "gpt4Toks = gpt4Tokenizer.encode(startingtext)\n",
    "\n",
    "# 2) back to text\n",
    "gpt4ReconText = gpt4Tokenizer.decode(gpt4Toks)\n",
    "\n",
    "# 3) then to bert tokens\n",
    "bertToks = bertTokenizer.encode(gpt4ReconText)\n",
    "\n",
    "# 4) show the reconstruction\n",
    "bertTokenizer.decode(bertToks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.251877Z",
     "start_time": "2025-12-22T19:18:03.250687Z"
    },
    "id": "E9RXStDEej1J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiuVkoJWakGa"
   },
   "source": [
    "# Possible annoyances and confusion in translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.258610Z",
     "start_time": "2025-12-22T19:18:03.255098Z"
    },
    "id": "-UxCgmWAzL9C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text contains 445 characters,\n",
      "              96 GPT4 tokens, and\n",
      "              160 Bert tokens.\n"
     ]
    }
   ],
   "source": [
    "# warning about sizes:\n",
    "txt = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n",
    "print(f'Text contains {len(txt)} characters,')\n",
    "print(f'              {len(gpt4Tokenizer.encode(txt))} GPT4 tokens, and')\n",
    "print(f'              {len(bertTokenizer.encode(txt))} Bert tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.273855Z",
     "start_time": "2025-12-22T19:18:03.271480Z"
    },
    "id": "2FZNon12Y3UT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction in BERT:\n",
      "  [101, 2707, 2203, 102]\n",
      "  [CLS] start end [SEP]\n",
      "\n",
      "Reconstruction in GPT4:\n",
      "  [2527, 881, 81923, 881, 4660, 319, 201, 408]\n",
      "  start\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# another source of confusion:\n",
    "txt = 'start\\r\\n\\r\\n\\r\\n\\n\\r\\n\\r\\n\\t\\t\\t\\n\\r\\n\\rend'\n",
    "# txt = 'start\\t\\t\\t\\t\\t\\t\\tend'\n",
    "# txt = 'start                    end'\n",
    "\n",
    "bertToks = bertTokenizer.encode(txt)\n",
    "gpt4Toks = gpt4Tokenizer.encode(txt)\n",
    "\n",
    "print(f'Reconstruction in BERT:\\n  {bertToks}\\n  {bertTokenizer.decode(bertToks)}\\n')\n",
    "print(f'Reconstruction in GPT4:\\n  {gpt4Toks}\\n  {gpt4Tokenizer.decode(gpt4Toks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:18:03.279494Z",
     "start_time": "2025-12-22T19:18:03.278280Z"
    },
    "id": "309X7W4O33Bi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN+6BPwzSfqO4pcbrsz5ArA",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
