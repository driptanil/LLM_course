{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B8BpAieRqI5"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Tokenization in BERT<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_4NUr5hhRqFc"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v1cyOt23Bqe"
   },
   "source": [
    "# Import BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IlLTVTpTBS75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drippy/.pyenv/versions/3.12.6/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVlg8p0x3KPP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em5ykH6t3KMg"
   },
   "source": [
    "# Inspect the tokenizer properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YFklMtfeCnJ9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_added_tokens_decoder',\n",
       " '_added_tokens_encoder',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_call_one',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_update_total_vocab_size',\n",
       " '_update_trie',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'basic_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'deprecation_warnings',\n",
       " 'do_basic_tokenize',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'ids_to_tokens',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'total_vocab_size',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size',\n",
       " 'wordpiece_tokenizer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the tokenizer info\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQnoY3PP3NZN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuWK93BA3NWd"
   },
   "source": [
    "# Check out some tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BVLQWI8s3RVJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chunk',\n",
       " 'rigorous',\n",
       " 'blaine',\n",
       " '198',\n",
       " 'peabody',\n",
       " 'slayer',\n",
       " 'dismay',\n",
       " 'brewers',\n",
       " 'nz',\n",
       " '##jer',\n",
       " 'det',\n",
       " '##glia',\n",
       " 'glover',\n",
       " 'postwar',\n",
       " 'int',\n",
       " 'penetration',\n",
       " 'sylvester',\n",
       " 'imitation',\n",
       " 'vertically',\n",
       " 'airlift',\n",
       " 'heiress',\n",
       " 'knoxville',\n",
       " 'viva',\n",
       " '##uin',\n",
       " '390',\n",
       " 'macon',\n",
       " '##rim',\n",
       " '##fighter',\n",
       " '##gonal',\n",
       " 'janice',\n",
       " '##orescence',\n",
       " '##wari',\n",
       " 'marius',\n",
       " 'belongings',\n",
       " 'leicestershire',\n",
       " '196',\n",
       " 'blanco',\n",
       " 'inverted',\n",
       " 'preseason',\n",
       " 'sanity',\n",
       " 'sobbing',\n",
       " '##due',\n",
       " '##elt',\n",
       " '##dled',\n",
       " 'collingwood',\n",
       " 'regeneration',\n",
       " 'flickering',\n",
       " 'shortest',\n",
       " '##mount',\n",
       " '##osi',\n",
       " 'feminism',\n",
       " '##lat',\n",
       " 'sherlock',\n",
       " 'cabinets',\n",
       " 'fumbled',\n",
       " 'northbound',\n",
       " 'precedent',\n",
       " 'snaps',\n",
       " '##mme',\n",
       " 'researching',\n",
       " '##akes',\n",
       " 'guillaume',\n",
       " 'insights',\n",
       " 'manipulated',\n",
       " 'vapor',\n",
       " 'neighbour',\n",
       " 'sap',\n",
       " 'gangster',\n",
       " 'frey',\n",
       " 'f1',\n",
       " 'stalking',\n",
       " 'scarcely',\n",
       " 'callie',\n",
       " 'barnett',\n",
       " 'tendencies',\n",
       " 'audi',\n",
       " 'doomed',\n",
       " 'assessing',\n",
       " 'slung',\n",
       " 'panchayat',\n",
       " 'ambiguous',\n",
       " 'bartlett',\n",
       " '##etto',\n",
       " 'distributing',\n",
       " 'violating',\n",
       " 'wolverhampton',\n",
       " '##hetic',\n",
       " 'swami',\n",
       " 'histoire',\n",
       " '##urus',\n",
       " 'liable',\n",
       " 'pounder',\n",
       " 'groin',\n",
       " 'hussain',\n",
       " 'larsen',\n",
       " 'popping',\n",
       " 'surprises',\n",
       " '##atter',\n",
       " 'vie',\n",
       " 'curt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = list(tokenizer.get_vocab().keys())\n",
    "all_tokens[20000:20100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2vUAJjP0CnHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2671"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "tokenizer.get_vocab()['science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oB_uEPlCnEj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1r6BXyv3ci4"
   },
   "source": [
    "# Tokenizing a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q-DdiCr23fBH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2671\n",
      "2671\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "\n",
    "res1 = tokenizer.convert_tokens_to_ids(word)\n",
    "res2 = tokenizer.get_vocab()[word]\n",
    "\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIZvZnNy3e-A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq2bgkEq3e7U"
   },
   "source": [
    "# Encoding a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IWjew3aOCnB3"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'science is great'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m'\u001b[39m\u001b[33mscience is great\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m res1 = tokenizer.convert_tokens_to_ids(text)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m res2 = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(res1)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(res2)\n",
      "\u001b[31mKeyError\u001b[39m: 'science is great'"
     ]
    }
   ],
   "source": [
    "text = 'science is great'\n",
    "\n",
    "res1 = tokenizer.convert_tokens_to_ids(text)\n",
    "res2 = tokenizer.get_vocab()[text]\n",
    "\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXXAfvB8GkdV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 101 is \"[CLS]\"\n",
      "Token 2671 is \"science\"\n",
      "Token 2003 is \"is\"\n",
      "Token 2307 is \"great\"\n",
      "Token 102 is \"[SEP]\"\n",
      "\n",
      "science is great\n",
      "[CLS] science is great [SEP]\n"
     ]
    }
   ],
   "source": [
    "# better method:\n",
    "res3 = tokenizer.encode(text)\n",
    "\n",
    "for i in res3:\n",
    "  print(f'Token {i} is \"{tokenizer.decode(i)}\"')\n",
    "\n",
    "# [CLS] = classification\n",
    "# [SEP] = sentence separation\n",
    "\n",
    "print('')\n",
    "print(tokenizer.decode(res3,skip_special_tokens=True))\n",
    "print(tokenizer.decode(res3,skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hhbmvRHvDVdV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [CLS] science is great [SEP] [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT adds [CLS]...[SEP] with each encode\n",
    "tokenizer.decode(tokenizer.encode(tokenizer.decode(tokenizer.encode( text ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhYhUdH2DLyI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rM1zNMeDLvX"
   },
   "source": [
    "# Calling the class directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jZZpL7xqDOWL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2671, 2003, 2307, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qJeimW3s2P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JHKZRrk3szm"
   },
   "source": [
    "# More on tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bbJ7u8mrHrrI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "  AI is both exciting and terrifying.\n",
      "\n",
      "Tokenized (segmented) sentence:\n",
      "  ['ai', 'is', 'both', 'exciting', 'and', 'terrifying', '.']\n",
      "  [9932, 2003, 2119, 10990, 1998, 17082, 1012]\n",
      "\n",
      "Encoded from the original text:\n",
      "  [101, 9932, 2003, 2119, 10990, 1998, 17082, 1012, 102]\n",
      "\n",
      "\n",
      "Decoded from token-wise encoding:\n",
      "  ai is both exciting and terrifying.\n",
      "\n",
      "Decoded from text encoding:\n",
      "  [CLS] ai is both exciting and terrifying. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'AI is both exciting and terrifying.'\n",
    "\n",
    "print('Original sentence:')\n",
    "print(f'  {sentence}\\n')\n",
    "\n",
    "# segment the text into tokens\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "print('Tokenized (segmented) sentence:')\n",
    "print(f'  {tokenized}')\n",
    "\n",
    "# encode the tokenized sentence\n",
    "ids_from_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(f'  {ids_from_tokens}\\n')\n",
    "\n",
    "# and finally, encode from the original sentence\n",
    "encodedText = tokenizer.encode(sentence)\n",
    "print('Encoded from the original text:')\n",
    "print(f'  {encodedText}\\n\\n')\n",
    "\n",
    "# now for decoding\n",
    "print('Decoded from token-wise encoding:')\n",
    "print(f'  {tokenizer.decode(ids_from_tokens)}\\n')\n",
    "\n",
    "print('Decoded from text encoding:')\n",
    "print(f'  {tokenizer.decode(encodedText)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tN-vdsOJAMb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrF0moQjxlYfOwvTAJKp57",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
